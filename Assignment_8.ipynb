{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习课上内容， 阅读相应论文。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答以下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.  What is autoencoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AutoEncoder是自动编码器，是一种数据的压缩算法，具有降维的作用。它的特点是编码器会创建一个隐藏层（或多个隐藏层）包含了输入数据含义的低维向量。然后有一个解码器，会通过隐藏层的低维向量重建输入数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What are the differences between greedy search and beam search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贪心搜索(greedy search)直接选择每个输出的最大概率，直到出现终结符或最大句子长度。\n",
    "\n",
    "集束搜索(beam search)可以认为是维特比算法的贪心形式，在维特比所有中由于利用动态规划导致当字典较大时效率低，而集束搜索使用beam size参数来限制在每一步保留下来的可能性词的数量。集束搜索是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the intuition of attention mechanism?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将有限的注意力集中在重点信息上，从而节省资源，快速获得最有效的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the disadvantage of word embeding introduced in previous lectures ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够根据上下文猜测意思，对于每一个单词都有唯一的一个embedding表示，但是无法识别一词多义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What is the architecture of ELMo model. (A brief description is enough)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elmo主要使用了一个两层双向的LSTM语言模型，左边输入的是句子的上文，右边输入的是句子的下文。训练好之后以三层内部状态的函数来表示词向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Compared to RNN,  what is the advantage of Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN并行计算能力比较差，时刻的计算依赖T-1时刻的隐层计算结果，而T-1时刻的计算依赖T-2时刻的隐层计算结果，这样就形成了所谓的序列依赖关系。而Transformer就不存在这种问题，计算效率也十分高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Why we use layer normalizaiton instead of batch normalization in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BN 的主要思想就是在每一层的每一批数据上进行归一化。LN 是在每一个样本上计算均值和方差。BN 的一个缺点是需要较大的 batchsize 才能合理估训练数据的均值和方差，这导致内存很可能不够用，同时它也很难应用在训练数据长度不同的模型上。Layer Normalization (LN) 的一个优势是不需要批训练，在单条数据内部就能归一化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Why we need position embedding in Transformer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "位置编码会在词向量中加入了单词的位置信息，这样Transformer就能区分不同位置的单词了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Briefly describe what is self-attention and what is multi-head attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-Attention则利用了Attention机制，计算每个单词与其他所有单词之间的关联，比如当翻译bank一词时，river一词就有较高的Attention score。Multi-head Attention其实就是多个Self-Attention结构的结合，每个head学习到在不同表示空间中的特征，两个head学习到的Attention侧重点可能略有不同，这样给了模型更大的容量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. What is the basic unit of GPT model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer norm, feed forward, and masked multi self attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Briefly descibe how to use GPT in other NLP tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT利用了Transformer网络代替了LSTM作为语言模型来更好的捕获长距离语言结构。对于分类任务直接讲文本输入即可；对于文本蕴涵任务，需要将前提和假设用一个Delim分割向量拼接后进行输入；对于文本相似度任务，在两个方向上都使用Delim拼接后，进行输入；对于像问答多选择的任务，就是将每个答案和上下文进行拼接进行输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. What is masked language model in BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT 提出一种新的预训练目标：遮蔽语言模型（masked language model，MLM），同时利用左侧和右侧的词语来克服单向性局限。MLM 随机遮蔽模型输入中的一些 token，目标在于仅基于遮蔽词的语境来预测其原始词汇id。在将单词序列输入给 BERT 之前，每个序列中有 15％ 的单词被mask token替换。 然后模型尝试基于序列中其他未被 mask 的单词的上下文来预测被掩盖的原单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 13. What are the inputs of BERT ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token Embeddings, Segment Embeddings, and Position Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 14. Briely descibe how to use BERT in other NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QA问答系统：首先往往会把比较长的文档切割成段落或者句子n-gram构成的语言片段，这些片段俗称Passage。将用户问句和候选passage作为Bert的输入，Bert做个分类，指出当前的Passage是否包括问句的正确答案，或者输出答案的起始终止位置。\n",
    "\n",
    "阅读理解：类似QA，但QA问题在找答案的时候，依赖的上下文更短小，参考的信息更局部一些，答案更表面化一些；而阅读理解任务，要正确定位答案，所参考的上下文范围可能会更长一些。\n",
    "\n",
    "抽取式摘要：用Transformer作为特征抽取器，并用Bert的预训练模型初始化Transformer参数，以这种方式构建一个句子的二分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 15. What are the differences between these three models: GPT, BERT, GPT2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结构：\n",
    "GPT2是单向Transformer，BERT基于双向Transformer。\n",
    "\n",
    "参数规模：\n",
    "GPT1.0的参数规模为1.1亿参数，12层transformer。Bert翻倍到3.4亿参数，24层transformer。GPT2.0的参数规模为15亿，48层transformer。\n",
    "\n",
    "目标任务：\n",
    "GPT1.0与BERT都是在目标任务上做有监督的微调。GPT2.0把第二阶段的Finetuning做有监督地下游NLP任务，换成了无监督地做下游任务。这样训练出来的语言模型，通用性好，覆盖几乎任何领域的内容。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
