{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.回答以下理论问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 请写一下TF-IDF的计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$tf_{i,j} = \\frac{n_{i,j}}{\\sum_k{n_{k.j}}}$\n",
    "\n",
    "$idf_i = log\\frac{|D|}{|{j:t_i \\in d_j }|}$\n",
    "\n",
    "where $n_{i,j}$ is the frequency of ith word in jth document, $|D|$ is number of total documentations, and $|{j:t_i \\in d_j }|$ is the number of document that contains the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. LDA算法的基本假设是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Per-document topic distributions: The topics of each document obey a probability distribution.\n",
    "\n",
    "- Per-topic word distributions: The words of each topic obey  a probability distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 在TextRank算法中构建图的权重是如何得到的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将每个句子看成图中的一个节点，若两个句子之间有相似性，认为对应的两个节点之间有一个无向有权边，权值是相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. 什么是命名实体识别？ 有什么应用场景？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "命名实体识别是将文本中的元素分成预先定义的类，如人名、地名、 机构名、时间、货币等等。作为自然语言的承载信息单位，命名实体识别属于文本信息处理的基础的研究领域，是信息抽取、信息检索、机器翻译、问答系统等多种自然语言处理技术中必不可少的组成部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.NLP主要有哪几类任务 ？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 序列标注：分词/POS Tag/NER/语义标注\n",
    "2. 分类任务：文本分类/情感计算\n",
    "3. 句子关系判断：Entailment/QA/自然语言推理\n",
    "4. 生成式任务：机器翻译/文本摘要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.实践题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 手动实现TextRank算法 (在新闻数据中随机提取100条新闻训练词向量和做做法测试）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 提示：\n",
    " 1. 确定窗口，建立图链接。   \n",
    " 2. 通过词向量相似度确定图上边的权重\n",
    " 3. 根据公式实现算法迭代(d=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "import jieba.posseg\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外，自本周（6月12日）起，除小米手机6等15款机型外，其余机型已暂停更新发布（含开发版/体验版内测，稳定版暂不受影响），以确保工程师可以集中全部精力进行系统优化工作 有人猜测这也是将精力主要用到M'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"../新华社数据.csv\", encoding='gb18030')[\"content\"][:100]\n",
    "sentences_list = []\n",
    "for new in news:\n",
    "    new = new.replace(\"\\r\\n\", \"\")\n",
    "    for sent in new.split(\"。\"):\n",
    "        sent = sent.strip()\n",
    "        if len(sent) > 4:\n",
    "            sentences_list.append(sent)\n",
    "sentences = \" \".join(sentences_list)\n",
    "sentences[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UndirectWeightedGraph:\n",
    "    d = 0.85\n",
    " \n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    " \n",
    "    def addEdge(self, start, end, weight):\n",
    "        # use a tuple (start, end, weight) instead of a Edge object\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    " \n",
    "    def rank(self):\n",
    "        ws = defaultdict(float)\n",
    "        outSum = defaultdict(float)\n",
    " \n",
    "        wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "        for n, out in self.graph.items():\n",
    "            ws[n] = wsdef\n",
    "            outSum[n] = sum((e[2] for e in out), 0.0)\n",
    " \n",
    "        # this line for build stable iteration\n",
    "        sorted_keys = sorted(self.graph.keys())\n",
    "        for x in range(10):  # 10 iters\n",
    "            for n in sorted_keys:\n",
    "                s = 0\n",
    "                for e in self.graph[n]:\n",
    "                    s += e[2] / outSum[e[1]] * ws[e[1]]\n",
    "                ws[n] = (1 - self.d) + self.d * s\n",
    " \n",
    "        (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])\n",
    " \n",
    "        for w in ws.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    " \n",
    "        for n, w in ws.items():\n",
    "            # to unify the weights, don't *100.\n",
    "            ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)\n",
    " \n",
    "        return ws\n",
    " \n",
    "\n",
    "class TextRank():\n",
    " \n",
    "    def __init__(self):\n",
    "        self.tokenizer = self.postokenizer = jieba.posseg.dt\n",
    "        self.span = 5\n",
    " \n",
    "    def pairfilter(self, wp):\n",
    "        return len(wp.word.strip()) >= 2\n",
    " \n",
    "    def textrank(self, sentence, topK=20, withWeight=True,):\n",
    "        g = UndirectWeightedGraph()\n",
    "        cm = defaultdict(int)\n",
    "        words = tuple(self.tokenizer.cut(sentence))\n",
    "        for i, wp in enumerate(words):\n",
    "            if self.pairfilter(wp):\n",
    "                for j in range(i + 1, i + self.span):\n",
    "                    if j >= len(words):\n",
    "                        break\n",
    "                    if not self.pairfilter(words[j]):\n",
    "                        continue\n",
    "                    cm[(wp.word, words[j].word)] += 1\n",
    " \n",
    "        for terms, w in cm.items():\n",
    "            g.addEdge(terms[0], terms[1], w)\n",
    "        nodes_rank = g.rank()\n",
    "        if withWeight:\n",
    "            tags = sorted(nodes_rank.items(), key=itemgetter(1), reverse=True)\n",
    "        else:\n",
    "            tags = sorted(nodes_rank, key=nodes_rank.__getitem__, reverse=True)\n",
    " \n",
    "        if topK:\n",
    "            return tags[:topK]\n",
    "        else:\n",
    "            return tags\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/ns/kcb8yjxs6qx72w25lyg4m_jc0000gn/T/jieba.cache\n",
      "Loading model cost 0.815 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('市场', 1.0),\n",
       " ('中国', 0.7492183274636033),\n",
       " ('我们', 0.7390225823952734),\n",
       " ('已经', 0.6816553844646037),\n",
       " ('进行', 0.6631682441471889),\n",
       " ('一个', 0.6327811875513467),\n",
       " ('可以', 0.5952540254557545),\n",
       " ('企业', 0.5855450134941426),\n",
       " ('以及', 0.5701328322041872),\n",
       " ('记者', 0.5466108676826839),\n",
       " ('目前', 0.527807516555052),\n",
       " ('融资', 0.5170155602716184),\n",
       " ('乐视', 0.509356316891696),\n",
       " ('可能', 0.4565071424754822),\n",
       " ('没有', 0.45561089947409117),\n",
       " ('冰架', 0.451081458299052),\n",
       " ('手机', 0.44503775258642686),\n",
       " ('其中', 0.42845878375930074),\n",
       " ('内容', 0.4256450315506142),\n",
       " ('相关', 0.4170464601023033)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextRank().textrank(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选做 1.  提取新闻人物里的对话。(使用以上提取小数据即可）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示：    \n",
    "1.寻找预料里具有表示说的意思。    \n",
    "2.使用语法分析提取句子结构。    \n",
    "3.检测谓语是否有表示说的意思。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择2. ： 电影评论分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个电影评论分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（采用爬虫技术爬取相关网页上的电影评论数据，例如猫眼电影评论，豆瓣电影评论）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.把所获得数据分解为训练集，验证集和测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.选用相应算法构建模型，并测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 选择3：文章自动续写"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这个作业中你要完成一个文章自动续写的模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.数据获取。（根据你的兴趣采用爬虫技术爬去相关网站上的文本数据内容：比如故事网站，小说网站等）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.选取模型，并训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.展示一些你模型的输出例子。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
